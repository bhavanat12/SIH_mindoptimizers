{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ﻿id\n",
      "1 dateAdded\n",
      "2 dateUpdated\n",
      "3 name\n",
      "4 asins\n",
      "5 brand\n",
      "6 categories\n",
      "7 primaryCategories\n",
      "8 imageURLs\n",
      "9 keys\n",
      "10 manufacturer\n",
      "11 manufacturerNumber\n",
      "12 reviews.date\n",
      "13 reviews.dateSeen\n",
      "14 reviews.didPurchase\n",
      "15 reviews.doRecommend\n",
      "16 reviews.id\n",
      "17 reviews.numHelpful\n",
      "18 reviews.rating\n",
      "19 reviews.sourceURLs\n",
      "20 reviews.text\n",
      "21 reviews.title\n",
      "22 reviews.username\n",
      "23 sourceURLs\n",
      "Processed 28333 lines.\n",
      "28332\n",
      "28332\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv',encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    \n",
    "    train_reviews=[]\n",
    "    train_coressponding_rating=[]\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "#             print(f'Column names are {\", \".join(row)}')\n",
    "            for j in range(len(row)):\n",
    "                print(j,row[j])\n",
    "            line_count += 1\n",
    "        else:\n",
    "#             print(f'\\t{row[0]} works in the {row[1]} department, and was born in {row[2]}.')\n",
    "            \n",
    "\n",
    "            train_reviews.append(row[20])\n",
    "            train_coressponding_rating.append(int(row[18]))\n",
    "            line_count += 1\n",
    "            \n",
    "        \n",
    "                  \n",
    "#             break\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    \n",
    "    \n",
    "print(len(train_reviews))\n",
    "print(len(train_coressponding_rating))\n",
    "print(type(train_coressponding_rating[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ﻿id\n",
      "1 dateAdded\n",
      "2 dateUpdated\n",
      "3 name\n",
      "4 asins\n",
      "5 brand\n",
      "6 categories\n",
      "7 primaryCategories\n",
      "8 imageURLs\n",
      "9 keys\n",
      "10 manufacturer\n",
      "11 manufacturerNumber\n",
      "12 reviews.date\n",
      "13 reviews.dateAdded\n",
      "14 reviews.dateSeen\n",
      "15 reviews.doRecommend\n",
      "16 reviews.id\n",
      "17 reviews.numHelpful\n",
      "18 reviews.rating\n",
      "19 reviews.sourceURLs\n",
      "20 reviews.text\n",
      "21 reviews.title\n",
      "22 reviews.username\n",
      "23 sourceURLs\n",
      "Processed 5001 lines.\n",
      "5000\n",
      "5000\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "with open('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv',encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    \n",
    "    test_reviews=[]\n",
    "    test_coressponding_rating=[]\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "#             print(f'Column names are {\", \".join(row)}')\n",
    "            for j in range(len(row)):\n",
    "                print(j,row[j])\n",
    "            line_count += 1\n",
    "        else:\n",
    "#             print(f'\\t{row[0]} works in the {row[1]} department, and was born in {row[2]}.')\n",
    "            \n",
    "\n",
    "            test_reviews.append(row[20])\n",
    "            test_coressponding_rating.append(int(row[18]))\n",
    "            line_count += 1\n",
    "            \n",
    "        \n",
    "                  \n",
    "#             break\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    \n",
    "    \n",
    "print(len(test_reviews))\n",
    "print(len(test_coressponding_rating))\n",
    "print(type(test_coressponding_rating[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews=train_reviews+test_reviews\n",
    "total_ratings=train_coressponding_rating+test_coressponding_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33332\n",
      "33332\n"
     ]
    }
   ],
   "source": [
    "print(len(total_reviews))\n",
    "print(len(total_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set:  26665\n",
      "Length of test set:  6667\n"
     ]
    }
   ],
   "source": [
    "rev_to_seq=[]\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(total_reviews)\n",
    "\n",
    "for i in range(len(total_reviews)):\n",
    "\n",
    "    rev_to_seq += t.texts_to_sequences([total_reviews[i]])\n",
    "    \n",
    "# print(len(rev_to_seq))\n",
    "\n",
    "x_train= rev_to_seq[:int(len(rev_to_seq)*0.8)]\n",
    "\n",
    "# x_train=rev_to_seq\n",
    "\n",
    "\n",
    "print(\"Length of training set: \",int(len(rev_to_seq)*0.8))\n",
    "\n",
    "x_test= rev_to_seq[int(len(rev_to_seq)*0.8):]\n",
    "\n",
    "\n",
    "print(\"Length of test set: \",int(len(x_test)))\n",
    "\n",
    "y_train= total_ratings[:int(len(rev_to_seq)*0.8)]\n",
    "# y_train= rating_corresponding\n",
    "\n",
    "\n",
    "y_test= total_ratings[int(len(rev_to_seq)*0.8):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences:  [3, 350, 157, 12, 40, 2, 39, 12, 1, 255, 8, 391, 95, 8, 1329, 1369, 2302, 24, 3, 20, 4, 200, 7, 4180, 12, 4643, 4, 236, 1, 68, 60]\n",
      "11334\n",
      "11334\n",
      "Sequences to words:  i order 3 of them and one of the item is bad quality is missing backup spring so i have to put a pcs of aluminum to make the battery work\n",
      "\n",
      "Min value-Negative review: [0, 0, 0, 0, 1] Max value-Positive review: [1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Sequences: \",x_train[0])\n",
    "# print(t.word_index)\n",
    "print(len(t.word_index))\n",
    "\n",
    "\n",
    "max_words=len(t.word_index)+20000\n",
    "reverse_word_map = dict(map(reversed, t.word_index.items()))\n",
    "\n",
    "print(max(reverse_word_map.keys()))\n",
    "\n",
    "print(\"Sequences to words: \",\" \".join([reverse_word_map[x] for x in x_train[0]]))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Min value-Negative review:\", min(y_train), \"Max value-Positive review:\", max(y_train))\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==1:\n",
    "        y_train[i]=[1,0,0,0,0]\n",
    "        \n",
    "    elif y_train[i]==2:\n",
    "        y_train[i]=[0,1,0,0,0]\n",
    "        \n",
    "    elif y_train[i]==3:\n",
    "        y_train[i]=[0,0,1,0,0]\n",
    "        \n",
    "    elif y_train[i]==4:\n",
    "        y_train[i]=[0,0,0,1,0]\n",
    "        \n",
    "    elif y_train[i]==5:\n",
    "        y_train[i]=[0,0,0,0,1]\n",
    "        \n",
    "    \n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]==1:\n",
    "        y_test[i]=[1,0,0,0,0]\n",
    "        \n",
    "    elif y_test[i]==2:\n",
    "        y_test[i]=[0,1,0,0,0]\n",
    "        \n",
    "    elif y_test[i]==3:\n",
    "        y_test[i]=[0,0,1,0,0]\n",
    "        \n",
    "    elif y_test[i]==4:\n",
    "        y_test[i]=[0,0,0,1,0]\n",
    "        \n",
    "    elif y_test[i]==5:\n",
    "        y_test[i]=[0,0,0,0,1]\n",
    "        \n",
    "\n",
    "y_test=np.array(y_test)\n",
    "y_train=np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length:  25.958034877179823\n",
      "Median sequence length:  17\n",
      "Max sequence length:  1559\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "average_length = np.mean([len(x) for x in x_train])\n",
    "median_length = sorted([len(x) for x in x_train])[len(x_train) // 2]\n",
    "max_length = max([len(x) for x in x_train])\n",
    "\n",
    "print(\"Average sequence length: \", average_length)\n",
    "print(\"Median sequence length: \", median_length)\n",
    "print(\"Max sequence length: \", max_length)\n",
    "\n",
    "max_sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_sequence_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (26665, 200)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape: ', x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Single layer LSTM example\n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "sl_model = Sequential()\n",
    "sl_model.add(Embedding(max_words, hidden_size))\n",
    "sl_model.add(LSTM(hidden_size, activation='tanh', dropout=0.2, recurrent_dropout=0.2))\n",
    "sl_model.add(Dense(5, activation='softmax'))\n",
    "sl_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\laxman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/6\n",
      "26665/26665 [==============================] - 112s 4ms/step - loss: 0.9237 - accuracy: 0.7022\n",
      "Epoch 2/6\n",
      "26665/26665 [==============================] - 110s 4ms/step - loss: 0.9093 - accuracy: 0.7026\n",
      "Epoch 3/6\n",
      "26665/26665 [==============================] - 111s 4ms/step - loss: 0.9084 - accuracy: 0.7026\n",
      "Epoch 4/6\n",
      "26665/26665 [==============================] - 110s 4ms/step - loss: 0.9070 - accuracy: 0.7030\n",
      "Epoch 5/6\n",
      "26665/26665 [==============================] - 110s 4ms/step - loss: 0.9069 - accuracy: 0.7032\n",
      "Epoch 6/6\n",
      "26665/26665 [==============================] - 110s 4ms/step - loss: 0.9061 - accuracy: 0.7038\n",
      "6667/6667 [==============================] - 7s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "\n",
    "sl_model.fit(x_train, y_train, epochs=epochs, shuffle=True)\n",
    "loss, acc = sl_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6667/6667 [==============================] - 7s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, acc = sl_model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single layer model -- ACC 0.6970151662826538 -- LOSS 0.8421730356129173\n"
     ]
    }
   ],
   "source": [
    "print('Single layer model -- ACC {} -- LOSS {}'.format(acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_model.save('./ratedmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "new_model = keras.models.load_model('./ratedmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03594206 0.02515308 0.04754182 0.18582895 0.70553404]]\n",
      "The rating predicted is  4\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "new_review = [\"The Baby Book: I first bought this book when my first child was born. Since then, I have bought it for baby showers and lent my copy to a friend (who eventually just kept it). Now that I am having a second child I just purchased another copy. I love the sections on bonding with your baby after birth and nursing. I wish that I had read both before the birth of my first child. There is also practical information and advice on child development, going back to work and nursing, childhood illness, and when to call your child's doctor. In short, I love this book.MeganMother of 1 and a half.\"]\n",
    "\n",
    "# new_review2= str(input())\n",
    "\n",
    "# new_review.append(new_review2)\n",
    "\n",
    "# new_review3= str(input())\n",
    "\n",
    "# new_review.append(new_review3)\n",
    "\n",
    "seq = t.texts_to_sequences(new_review)\n",
    "padded = sequence.pad_sequences(seq, maxlen=200, padding='post', truncating='post')\n",
    "pred = new_model.predict(padded)\n",
    "print(pred)\n",
    "\n",
    "for prediction in pred:\n",
    "    k=np.argmax(prediction)\n",
    "    \n",
    "    print(\"The rating predicted is \",k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
