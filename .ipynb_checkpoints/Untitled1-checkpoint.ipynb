{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "Length of training set:  80000\n",
      "Length of test set:  20000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "filename = \"train.ft.txt\"\n",
    "\n",
    "filereader=open(filename,'r',encoding='utf8')\n",
    "\n",
    "reviews=[]\n",
    "rating_corresponding=[]\n",
    "\n",
    "i=0\n",
    "\n",
    "for line in filereader:\n",
    "    line=str(line)\n",
    "    reviews.append(line)\n",
    "    i += 1\n",
    "    \n",
    "    if(i==100000):\n",
    "        break    \n",
    "    \n",
    "    \n",
    "random.shuffle(reviews)\n",
    "\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(int(reviews[i][9])==1):\n",
    "        rating_corresponding.append(0)\n",
    "    elif (int(reviews[i][9])==2):\n",
    "        rating_corresponding.append(1)\n",
    "        \n",
    "    reviews[i]=reviews[i][11:]\n",
    "    \n",
    "#     print(line[11:])\n",
    "#     break\n",
    "\n",
    "    \n",
    "print(len(reviews))\n",
    "print(len(rating_corresponding))\n",
    "# print(rating_corresponding)\n",
    "\n",
    "rev_to_seq=[]\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(reviews)\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "\n",
    "    rev_to_seq += t.texts_to_sequences([reviews[i]])\n",
    "    \n",
    "# print(len(rev_to_seq))\n",
    "\n",
    "x_train= rev_to_seq[:int(len(rev_to_seq)*0.8)]\n",
    "\n",
    "# x_train=rev_to_seq\n",
    "\n",
    "\n",
    "print(\"Length of training set: \",int(len(rev_to_seq)*0.8))\n",
    "\n",
    "x_test= rev_to_seq[int(len(rev_to_seq)*0.8):]\n",
    "\n",
    "\n",
    "print(\"Length of test set: \",int(len(x_test)))\n",
    "\n",
    "y_train= rating_corresponding[:int(len(rev_to_seq)*0.8)]\n",
    "# y_train= rating_corresponding\n",
    "\n",
    "\n",
    "y_test= rating_corresponding[int(len(rev_to_seq)*0.8):]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences:  [852, 49, 180, 5, 647, 4, 255, 6, 22492, 3, 63, 68, 32, 105, 6, 76, 190, 146, 9, 149, 24, 3, 304, 3, 249, 20, 38, 7, 138, 313, 17, 7, 3155, 42, 10, 21, 414, 22, 4, 30, 38, 29, 291, 15, 251, 46, 16, 6, 22492, 5, 38, 263, 93, 24, 14, 1035, 53, 68, 8, 24, 53, 68, 14, 67, 3, 99]\n",
      "121536\n",
      "121536\n",
      "Sequences to words:  scary it's hard to pick a favorite of koontz's i really do like most of them here again is another one i enjoyed i may have read it years ago but it sticks out in my mind as a great read if you're not sure what book of koontz's to read almost any one you touch will do this one will do you well i think\n",
      "\n",
      "Min value-Negative review: 0 Max value-Positive review: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequences: \",x_train[0])\n",
    "# print(t.word_index)\n",
    "print(len(t.word_index))\n",
    "\n",
    "\n",
    "max_words=len(t.word_index)+20000\n",
    "reverse_word_map = dict(map(reversed, t.word_index.items()))\n",
    "\n",
    "print(max(reverse_word_map.keys()))\n",
    "\n",
    "print(\"Sequences to words: \",\" \".join([reverse_word_map[x] for x in x_train[0]]))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Min value-Negative review:\", min(y_train), \"Max value-Positive review:\", max(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length:  80.841575\n",
      "Median sequence length:  73\n",
      "Max sequence length:  242\n"
     ]
    }
   ],
   "source": [
    "average_length = np.mean([len(x) for x in x_train])\n",
    "median_length = sorted([len(x) for x in x_train])[len(x_train) // 2]\n",
    "max_length = max([len(x) for x in x_train])\n",
    "\n",
    "print(\"Average sequence length: \", average_length)\n",
    "print(\"Median sequence length: \", median_length)\n",
    "print(\"Max sequence length: \", max_length)\n",
    "\n",
    "max_sequence_length = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_sequence_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (80000, 180)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape: ', x_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6fbbd8dbbb60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msl_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msl_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0msl_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msl_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_words' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Single layer LSTM example\n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "sl_model = Sequential()\n",
    "sl_model.add(Embedding(max_words, hidden_size))\n",
    "sl_model.add(LSTM(hidden_size, activation='tanh', dropout=0.2, recurrent_dropout=0.2))\n",
    "sl_model.add(Dense(1, activation='sigmoid'))\n",
    "sl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "79232/80000 [============================>.] - ETA: 6s - loss: 0.6929 - acc: 0.5122"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "\n",
    "sl_model.fit(x_train, y_train, epochs=epochs, shuffle=True)\n",
    "loss, acc = sl_model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = Sequential()\n",
    "# d_model.add(Embedding(max_words, hidden_size))\n",
    "# d_model.add(LSTM(hidden_size, activation='tanh', dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "# d_model.add(LSTM(hidden_size, activation='tanh', dropout=0.2, recurrent_dropout=0.2))\n",
    "# d_model.add(Dense(1, activation='sigmoid'))\n",
    "# d_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model.fit(x_train, y_train, epochs=epochs, shuffle=True)\n",
    "# d_loss, d_acc = d_model.evaluate(x_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single layer model -- ACC 0.9116 -- LOSS 0.22786018221378326\n"
     ]
    }
   ],
   "source": [
    "print('Single layer model -- ACC {} -- LOSS {}'.format(acc, loss))\n",
    "# print('Double layer model -- ACC {} -- LOSS {}'.format(d_acc, d_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01359553]]\n"
     ]
    }
   ],
   "source": [
    "new_complaint = [\"DVD Player crapped out after one year: I also began having the incorrect disc problems that I've read about on here. The VCR still works, but hte DVD side is useless. I understand that DVD players sometimes just quit on you, but after not even one year? To me that's a sign on bad quality. I'm giving up JVC after this as well. I'm sticking to Sony or giving another brand a shot.\"]\n",
    "seq = t.texts_to_sequences(new_complaint)\n",
    "padded = sequence.pad_sequences(seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "pred = sl_model.predict(padded)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_model.save('./savedmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "new_model = keras.models.load_model('./savedmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had a bad experience with it\n",
      "good one\n",
      "[[0.9380749]]\n",
      "0  review is a positive review\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "new_review = [\"The Baby Book: I first bought this book when my first child was born. Since then, I have bought it for baby showers and lent my copy to a friend (who eventually just kept it). Now that I am having a second child I just purchased another copy. I love the sections on bonding with your baby after birth and nursing. I wish that I had read both before the birth of my first child. There is also practical information and advice on child development, going back to work and nursing, childhood illness, and when to call your child's doctor. In short, I love this book.MeganMother of 1 and a half.\"]\n",
    "\n",
    "new_review2= str(input())\n",
    "\n",
    "new_review.append(new_review2)\n",
    "\n",
    "new_review3= str(input())\n",
    "\n",
    "new_review.append(new_review3)\n",
    "\n",
    "seq = t.texts_to_sequences([new_review2])\n",
    "padded = sequence.pad_sequences(seq, maxlen=200, padding='post', truncating='post')\n",
    "pred = new_model.predict(padded)\n",
    "print(pred)\n",
    "\n",
    "\n",
    "for result in range(len(pred)):\n",
    "    if pred[result]>0.5:\n",
    "        print(result,\" review is a positive review\")\n",
    "    \n",
    "    else:\n",
    "        print(result,\" review is a negative review\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
